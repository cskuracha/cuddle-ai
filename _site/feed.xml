<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2021-04-08T23:30:25+05:30</updated><id>http://localhost:4000/feed.xml</id><title type="html">cuddle-ai</title><author><name>Chaitanya Sagar Kuracha</name></author><entry><title type="html">Neural Networks - History</title><link href="http://localhost:4000/2021/04/08/Neural-Networks-History.html" rel="alternate" type="text/html" title="Neural Networks - History" /><published>2021-04-08T00:00:00+05:30</published><updated>2021-04-08T00:00:00+05:30</updated><id>http://localhost:4000/2021/04/08/Neural-Networks-History</id><content type="html" xml:base="http://localhost:4000/2021/04/08/Neural-Networks-History.html">&lt;p&gt;In this article, I want to give brief introduction to Neural Networks.&lt;/p&gt;

&lt;p&gt;First Neural Network was designed by Frank Rosenblatt in 1957 and is called perceptron. Concept is loosely inspired from Biological neurons.&lt;/p&gt;

&lt;p&gt;Concepts of Neural networks are loosely inspired from Biological neurons (Biological neurons are more complicated).&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;img src=&quot;/assets/images/2021-04-08-Neural-Networks-History-1_1.jpg&quot; alt=&quot;Neuron&quot; /&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;(Image from https://en.wikipedia.org/wiki/Artificial_neuron#/media/File:Neuron3.svg)&lt;/p&gt;

&lt;p&gt;On highlevel, neurons contains three parts,&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Dendrites  -&amp;gt; Dendrites collects the impulses. Size and Width of Dendrites, determine the strength of impulse&lt;/li&gt;
  &lt;li&gt;Cell body (nucleus) -&amp;gt; Nucleus collect the impulses from all the Dendrites and process information&lt;/li&gt;
  &lt;li&gt;Axoms -&amp;gt; Axioms will send the information to other neurons&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Biological neurons cannot exist on their own but exists in networks. Output of one neuron connects to dendrites of other neurons and forms the networks&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;img src=&quot;/assets/images/2021-04-08-Neural-Networks-History-1_2.png&quot; alt=&quot;Neural Neuron&quot; /&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In Artificial Neural networks, we will pass inputs, x1, x2, x3,â€¦ xn with w1, w2, w3, â€¦ wn as corresponding weights to the neurons. These inputs and weights are weighted and summed and passed thru activating function. This function process the data and send the output.&lt;/p&gt;

&lt;p&gt;In 1986, Hilton and others submitted work on how to train Artificial neural networks and is called as Back Propagation algorithm. During this time, neural networks failed when number of hidden layers (between input and output layers) are more.&lt;/p&gt;

&lt;p&gt;Neural Networks gained popularity again in 2012 with ImageNet competition.&lt;/p&gt;

&lt;p&gt;Today artificial neural networks are applied in different Voice assistants, Self driving cars etc.,&lt;/p&gt;

&lt;p&gt;Hope you like this article and please share your feedback.&lt;/p&gt;</content><author><name>Chaitanya Sagar Kuracha</name></author><summary type="html">In this article, I want to give brief introduction to Neural Networks.</summary></entry><entry><title type="html">Biological Neurons</title><link href="http://localhost:4000/2021/04/08/Biological-Neurons.html" rel="alternate" type="text/html" title="Biological Neurons" /><published>2021-04-08T00:00:00+05:30</published><updated>2021-04-08T00:00:00+05:30</updated><id>http://localhost:4000/2021/04/08/Biological-Neurons</id><content type="html" xml:base="http://localhost:4000/2021/04/08/Biological-Neurons.html">&lt;p&gt;In this article, we will look into how Biological Neurons work and how it is different from Artificial Neurons&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;img src=&quot;/assets/images/2021-04-08-Biological-Neurons-1.jpg&quot; alt=&quot;Neuron&quot; /&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;When we observe a single neuron, some dendrites will be of more thinkness than other dendrites. This allows to pass more impules to nucleus than other dendrites.
Once we have enough impules from dendrites then neurons sends the output to axioms, else neurons will not be activated.&lt;/p&gt;

&lt;p&gt;Also, if we observe the scans of kids from newborn to 6 years, we see the connections betweens nerons is increasing and becoming dense upto 2 years. After 2 years connections are pruning.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;img src=&quot;https://cdn.filestackcontent.com/rotate=deg:exif/resize=width:1148,height:500,fit:crop/FI20xFeyTKuwS5mQoR9g&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Kids start learning about language , shapes, objects etc., durning early age and build the neurons. After some age, these neurons starts pruning as few of the neurons (experience) will be redundant or random connections. Also, when we observe the dendrites, we can see that few dendrites are more think than others. Dendrites with more thickness will give more impules than the less thick ones.&lt;/p&gt;

&lt;p&gt;Durning the initial years, kids consume lots of data like visual data, audio, sensory data etc., Since child consumes lots of data, our biological algorithm creates new connections based on the data that child has consumed. Kids born on different countries, conditions or circumstances will have different neuron connections. For example, a kid born in India and Canada will have different connections established because of their language / culture / parents etc.,&lt;/p&gt;

&lt;h4 id=&quot;how-thickness-of-connections-are-formed&quot;&gt;How thickness of connections are formed?&lt;/h4&gt;

&lt;p&gt;When a kid touches a hot object, it gives child a pain and forms a strong neural connection, so that when there is a hot object near to kid, it gives a strong signal that dont touch the object. 
Similarly, when a kid is born in Canada, he knows how snow looks and feels but kid born in India dont have that connection available.&lt;/p&gt;

&lt;p&gt;We can write activation function as,&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;img src=&quot;/assets/images/2021-04-08-Biological-Neurons-2.jpg&quot; alt=&quot;Neuron&quot; /&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This function is similar to Logistic Regression&lt;/p&gt;

&lt;p&gt;When there is no connection between neurons, then weight of that connection is 0&lt;/p&gt;

&lt;h4 id=&quot;bottomline&quot;&gt;Bottomline:&lt;/h4&gt;

&lt;p&gt;When we learn a new thing, a new connection will be formed between set of neurons and weights / thickness will be established.&lt;/p&gt;

&lt;p&gt;Hope you like this article and please share your feedback.&lt;/p&gt;</content><author><name>Chaitanya Sagar Kuracha</name></author><summary type="html">In this article, we will look into how Biological Neurons work and how it is different from Artificial Neurons</summary></entry><entry><title type="html">Distance Measures</title><link href="http://localhost:4000/2021/04/04/Distance-Measures.html" rel="alternate" type="text/html" title="Distance Measures" /><published>2021-04-04T00:00:00+05:30</published><updated>2021-04-04T00:00:00+05:30</updated><id>http://localhost:4000/2021/04/04/Distance-Measures</id><content type="html" xml:base="http://localhost:4000/2021/04/04/Distance-Measures.html">&lt;p&gt;In this article, we will look into different distance measures we will use in Machine Learning.&lt;/p&gt;

&lt;h3 id=&quot;1-euclidean-distance-l2&quot;&gt;1. Euclidean Distance (L2):&lt;/h3&gt;

&lt;p&gt;Euclidean Distance is most frequently used distance measure.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;img src=&quot;/assets/images/2021-04-04-distance-measures-1.jpg&quot; alt=&quot;Graph&quot; /&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;If we observe the above picture, variable â€˜dâ€™ is called the Euclidean distance (L2). It is called the shortest distance from X1 to X2.&lt;/p&gt;

&lt;p&gt;In above picture, we have two features, f1 and f2 and two datapoints in 2D, X1 and X2.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;img src=&quot;/assets/images/2021-04-04-distance-measures-3.jpg&quot; alt=&quot;Euclidean Measure&quot; /&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Euclidean distance can be formulated as&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;img src=&quot;/assets/images/2021-04-04-distance-measures-2.jpg&quot; alt=&quot;Euclidean Measure&quot; /&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;2-manhattan-distance-l1&quot;&gt;2. Manhattan Distance (L1):&lt;/h3&gt;

&lt;p&gt;Manhattan Distance is next frequently used and simple distance measure.&lt;/p&gt;

&lt;p&gt;From above graph, Manhattan distance can be formulated as,&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;img src=&quot;/assets/images/2021-04-04-distance-measures-4.jpg&quot; alt=&quot;Manhattan Measure&quot; /&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;3-minkowski-distance-lp&quot;&gt;3. Minkowski Distance (LP):&lt;/h3&gt;

&lt;p&gt;Minkowski Distance is the generalized distance measure and can be formulated as,&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;img src=&quot;/assets/images/2021-04-04-distance-measures-5.jpg&quot; alt=&quot;Minkowski Measure&quot; /&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;When p=1 in Minkowski Distance, it becomes &lt;em&gt;Manhattan Distance&lt;/em&gt; and when p=2, it becomes &lt;em&gt;Euclidean distance&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Manhattan distance is also called as L1-Norm and Euclidean distance is called as L2-Norm&lt;/p&gt;

&lt;h3 id=&quot;4-hamming-distance&quot;&gt;4. Hamming Distance:&lt;/h3&gt;

&lt;p&gt;Hamming distance is widely used in text processing and it gives us the # of difference of values between dimensions in vectors.
Hamming distance will  be used in Gene sequence.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;img src=&quot;/assets/images/2021-04-04-distance-measures-6.jpg&quot; alt=&quot;Hamming Distance&quot; /&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Above sequence generates value of 2 as there is difference of values in 7 and 10 dimensions&lt;/p&gt;

&lt;p&gt;There are other measures as well and we will understand about few of them in the future. One such important and widely used measure is &lt;em&gt;Cosine Distance&lt;/em&gt; and &lt;em&gt;Cosine similarity&lt;/em&gt;, we will see this in upcoming articles.&lt;/p&gt;

&lt;h3 id=&quot;bottomline&quot;&gt;Bottomline:&lt;/h3&gt;

&lt;p&gt;There are many distance measures that are used in Machine Learning and few of them are Manhattan distance, Euclidean distance, Minkowski distance and Hamming distance. 
There are other distance measures also which are widely used and we will see them in future articles.&lt;/p&gt;

&lt;p&gt;Hope you like this article and please share your feedback.&lt;/p&gt;</content><author><name>Chaitanya Sagar Kuracha</name></author><summary type="html">In this article, we will look into different distance measures we will use in Machine Learning.</summary></entry><entry><title type="html">Classification and Regression</title><link href="http://localhost:4000/2021/04/04/Classification-and-regression.html" rel="alternate" type="text/html" title="Classification and Regression" /><published>2021-04-04T00:00:00+05:30</published><updated>2021-04-04T00:00:00+05:30</updated><id>http://localhost:4000/2021/04/04/Classification-and-regression</id><content type="html" xml:base="http://localhost:4000/2021/04/04/Classification-and-regression.html">&lt;p&gt;In this article, we will look into two different kinds of Machine learning algorithms, Classification and Regression models.&lt;/p&gt;

&lt;p&gt;The main objective of Machine learning model is to find a function f, such that when a query point (x_q) is passed thru the function f, it will return desired output (y_q_^)&lt;/p&gt;

&lt;h3 id=&quot;classification&quot;&gt;Classification:&lt;/h3&gt;

&lt;p&gt;Lets consider an example for understanding the classification problems. Say, we want to classify reviews in an eCommerce website and typically we need to classify each review as positive review, negative review or neutral review. 
This type of problem is called Classification problem.&lt;/p&gt;

&lt;p&gt;Here when we look closely, we are classifying the datapoints into set of classes like positive, negative, neutral etc.. This kind of technique is called Classification.&lt;/p&gt;

&lt;p&gt;We can interpret classification dataset as below:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;img src=&quot;/assets/images/2021-04-04-Classifiction-and-regression-1.jpg&quot; alt=&quot;Classification&quot; /&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;where x is input data and y is the output class.&lt;/p&gt;

&lt;h3 id=&quot;regression&quot;&gt;Regression:&lt;/h3&gt;

&lt;p&gt;Consider this example for understanding regression problems. Say, we want to predict the heights of students in a school. Here our predicted value is a real number since height is a real number.&lt;/p&gt;

&lt;p&gt;We can interpret regression dataset as below:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;img src=&quot;/assets/images/2021-04-04-Classifiction-and-regression-2.jpg&quot; alt=&quot;Regression&quot; /&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;where x is input data and y is a real valued number&lt;/p&gt;

&lt;h3 id=&quot;did-you-observe-difference-between-classification-and-regression-&quot;&gt;Did you observe difference between Classification and Regression ?&lt;/h3&gt;

&lt;p&gt;If you observe closely on the dataset interpretations for Classification and Regression,  we can see the difference in the output variable â€˜yâ€™.&lt;/p&gt;

&lt;p&gt;In Classification, â€˜yâ€™ is a finite number where as in Regression, â€˜yâ€™ is a real valued number, which is a difference on very high level. We will understand more differences as we go on.&lt;/p&gt;

&lt;h3 id=&quot;bottomline&quot;&gt;Bottomline:&lt;/h3&gt;

&lt;p&gt;On simple and very highlevel terms, difference between Classification and Regression comes to difference between finite integer and Real valued number ðŸ˜ƒ&lt;/p&gt;

&lt;p&gt;Hope you like this article and feel free to share your comments.&lt;/p&gt;</content><author><name>Chaitanya Sagar Kuracha</name></author><summary type="html">In this article, we will look into two different kinds of Machine learning algorithms, Classification and Regression models.</summary></entry><entry><title type="html">Test Post</title><link href="http://localhost:4000/2021/04/03/Test-Post.html" rel="alternate" type="text/html" title="Test Post" /><published>2021-04-03T00:00:00+05:30</published><updated>2021-04-03T00:00:00+05:30</updated><id>http://localhost:4000/2021/04/03/Test-Post</id><content type="html" xml:base="http://localhost:4000/2021/04/03/Test-Post.html">&lt;p&gt;This is a Test Page&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-math&quot;&gt;a+b=c
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;{(x_i,y_i)}&lt;/p&gt;</content><author><name>Chaitanya Sagar Kuracha</name></author><summary type="html">This is a Test Page a+b=c {(x_i,y_i)}</summary></entry><entry><title type="html">Different techniques to convert words to vectors</title><link href="http://localhost:4000/2021/04/03/techniques-to-convert-words-to-vectors.html" rel="alternate" type="text/html" title="Different techniques to convert words to vectors" /><published>2021-04-03T00:00:00+05:30</published><updated>2021-04-03T00:00:00+05:30</updated><id>http://localhost:4000/2021/04/03/techniques-to-convert-words-to-vectors</id><content type="html" xml:base="http://localhost:4000/2021/04/03/techniques-to-convert-words-to-vectors.html">&lt;p&gt;In Machine Learning, we want to convert words into vectors to fully utilize the mathematical functions for the model.&lt;/p&gt;

&lt;p&gt;There are various techniques using which we can convert words into vectors. We will look into those techniques in this article.&lt;/p&gt;

&lt;p&gt;Below are few techniques which can convert words to vectors:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Bag of Words (BOW)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;TF-IDF Vectorizer&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Word2Vec&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;input-text&quot;&gt;Input Text&lt;/h4&gt;

&lt;p&gt;Lets say we have 4 sentences which we want to convert to vectors:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt; This headphone is amazing

&amp;gt; Noise Cancelling in this headphone is really awesome

&amp;gt; This headphone is not good

&amp;gt; This headphone is TrulyWireless and works as described in the description
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;bag-of-words-bow&quot;&gt;Bag of Words (BoW)&lt;/h4&gt;

&lt;p&gt;This is the simplest way to convert text to a vector. Basic idea of this technique is store all unique words in a list and for each sentence we will create a vector of length same as unique words. This creates a &lt;em&gt;sparse vector&lt;/em&gt; where most of the values in the vector are zeros.&lt;/p&gt;

&lt;p&gt;From our above example, our Bag of unique words contains:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;{This, headphone, is, amazing, Noise, Cancelling, in, really, awesome, not, good, TrulyWireless, and, works, as, described, the, description}&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Here we have 18 unique words in the bag, so we will represent each of the sentense in a vector of length 18.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Representation of sentence 1 -&amp;gt; This headphone is amazing -&amp;gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;[1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0]&lt;/code&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Similarly other sentences can be represented as,&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Noise Cancelling in this headphone is really awesome -&amp;gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;[1,1,1,0,1,1,1,1,1,0,0,0,0,0,0,0,0,0]&lt;/code&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;This headphone is not good -&amp;gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;[1,1,1,0,0,0,0,0,0,1,1,0,0,0,0,0,0]&lt;/code&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Same is repeated for other sentence&lt;/p&gt;

&lt;h4 id=&quot;tf-idf-vectorizer&quot;&gt;TF-IDF Vectorizer&lt;/h4&gt;

&lt;p&gt;Bag of Words simply convert each word into a vector without considering the importance of the word. In TF-IDF, we will consider the frequency of word into consideration.&lt;/p&gt;

&lt;h5 id=&quot;tf-term-frequency&quot;&gt;TF (Term Frequency):&lt;/h5&gt;

&lt;p&gt;How often a word occurs in corpus. If a word occurs multiple times, then TF of the word will be high&lt;/p&gt;

&lt;h5 id=&quot;idf-inverse-document-frequency&quot;&gt;IDF (Inverse Document Frequency):&lt;/h5&gt;

&lt;p&gt;How rare a word occurs in corpus. If a word is rare, then the IDF of the word will be high&lt;/p&gt;

&lt;p&gt;By combining TF-IDF, we are giving importance to rare word in corpus and frequency of occurance in current sentence&lt;/p&gt;

&lt;p&gt;From our above example, our unique words are:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;{This, headphone, is, amazing, Noise, Cancelling, in, really, awesome, not, good, TrulyWireless, and, works, as, described, the, description}&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;For computing the TF-IDF of a word in Sentence 1, consider â€˜Thisâ€™ word&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;TF&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;This&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;IDF&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;This&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;TF&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;IDF&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;This&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;TF&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;This&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;IDF&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;This&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Here word â€˜Thisâ€™ occurs very frequently in the document corpus and is not a important word and hence TF-IDF of â€˜Thisâ€™ is 0&lt;/p&gt;

&lt;p&gt;so, we will update TF-IDF of Sentence 1 as -&amp;gt; This headphone is amazing -&amp;gt;&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;TF&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;IDF&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;This&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;TF&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;IDF&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;headphone&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;TF&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;IDF&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;is&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;TF&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;IDF&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;amazing&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;word2vec&quot;&gt;Word2Vec&lt;/h4&gt;

&lt;p&gt;Word2Vec considers the semantic meaning and generates vectors for each word. Word2Vec considers the context and relation between the words. 
Distance between vectors of similar words is less. For example, distance between vectors generated by Word2Vec for King and Queen will be similar to distance between vectors for Man and Woman.
Word2Vec learns the vectors from large document corpus.&lt;/p&gt;

&lt;h4 id=&quot;conclusion&quot;&gt;Conclusion:&lt;/h4&gt;

&lt;p&gt;Using above techniques, we can convert words into vectors on which we can apply transformation that are useful for machine learning.&lt;/p&gt;

&lt;p&gt;I will try to create blog for each of the techniques more elaborately.&lt;/p&gt;</content><author><name>Chaitanya Sagar Kuracha</name></author><summary type="html">In Machine Learning, we want to convert words into vectors to fully utilize the mathematical functions for the model.</summary></entry></feed>